---
title: "Deep neutral networks -- Joanna Wojtukiewicz"
author: "Joanna Wojtukiewicz"
date: "7 02 2021"
output:
  pdf_document: default
  html_document: default
---

## Installation
1. Library installation
```{r install, eval=FALSE, include=TRUE}
#install.packages('keras')
```

```{r load_keras, message=FALSE, warning=FALSE}
library(keras)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
```

```{r install_tf, eval = FALSE, include = TRUE}
install_keras()
```

## Fashion MNIST data set

### Loading data set
```{r load_data}
fashion_mnist <- dataset_fashion_mnist()
c(x_train, y_train) %<-% fashion_mnist$train # Train set features, train set labels
c(x_test, y_test) %<-% fashion_mnist$test # Test set features, test set labels
```

### Visualize the 9 first items from training set

```{r, visualize_digits}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) { 
  im <- x_train[i,,]
  im <- t(apply(im, 2, rev)) 
  image(1:28, 1:28, im, col = gray((255:0) / 255), 
        xaxt = 'n', main = paste(y_train[i]))
}
```

### Data prepare
To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1

```{r x_data_prepare}
# Reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 28 * 28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28 * 28))
# Rescale
x_train <- x_train / 255
x_test <- x_test / 255
```

```{r y_data_prepare}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

9 first encoded items from trainig set are below (the first column corresponds to zero, second to one, etc.):
```{r}
head(y_train, 9)
```

### Defining the model
The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers.

```{r model}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = 28 * 28) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
```
The input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function. Softmax activation function takes as input a vector of $K$ real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers $$f_i(\pmb z) = \frac{e^{z_{i}}}{\sum_{j=1}^Ke^{z_{j}}}.$$

### Summary the model
```{r model_summary}
summary(model)
```

### Compile the model
* Loss function -- This measures how accurate the model is during training. We want to minimize this function to 'steer' the model in the right direction.
* Optimizer -- This is how the model is updated based on the data it sees and its loss function.
* Metrics -- Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the digits that are correctly classified.

```{r model_compile}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = 'accuracy')
```

### Training the model
```{r}
model %>% fit(x_train, 
              y_train, 
              epochs = 50, # Number of epochs
              batch_size = 128, # Size of batch in single step
              validation_split = 0.2 # Percent of data in validation sets
) -> model_dnn 
plot(model_dnn)
```

### Evaluate the model
```{r}
model %>% evaluate(x_train, y_train) # Evaluate the model’s performance on the train data
model %>% evaluate(x_test, y_test) # Evaluate the model’s performance on the test data
```

### Predictions
```{r}
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% predict_classes(x_test) -> predicted_items # Predicted digits on test data
```

```{r}
par(mfcol = c(5, 5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) { 
  img <- fashion_mnist$test$x[i, , ]
  img <- t(apply(img, 2, rev))
  if (predicted_items[i] == fashion_mnist$test$y[i]) {
    color <- '#008800' 
  } else {
    color <- '#bb0000'
  }
  image(1:28, 1:28, img, col = gray((255:0) / 255), xaxt = 'n', yaxt = 'n',
        main = paste0(predicted_items[i], ' (',
                      fashion_mnist$test$y[i], ')'),
        col.main = color)
}
```

### Confusion matrix
```{r}
data.frame(table(predicted_items, fashion_mnist$test$y)) %>% 
  setNames(c('Prediction', 'Reference', 'Freq')) %>% 
  mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table

conf_table %>% 
  ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) + 
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) + 
  scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
  guides(alpha = FALSE) + 
  theme_bw() + 
  ylim(rev(levels(conf_table$Reference)))
```